{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5881b7e-1ba8-410f-9231-9536e7505510",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Neste notbook são feitas as configurações adicionais relacionadas ao servidor Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e93d24-2def-4861-9ceb-2fb31b0a7ef1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.logging.log4j--log4j-slf4j-impl--org.apache.logging.log4j__log4j-slf4j-impl__2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/databricks/driver/kafka_2.12-3.5.1/libs/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[2023-10-04 23:14:50,586] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\n[2023-10-04 23:14:52,289] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\n[2023-10-04 23:14:52,811] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\n[2023-10-04 23:14:52,823] INFO starting (kafka.server.KafkaServer)\n[2023-10-04 23:14:52,824] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)\n[2023-10-04 23:14:52,978] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)\n[2023-10-04 23:14:53,007] INFO Client environment:zookeeper.version=3.6.2--803c7f1a12f85978cb049af5e4ef23bd8b688715, built on 09/04/2020 12:44 GMT (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,008] INFO Client environment:host.name=1004-210429-gf1iljoc-10-172-194-117 (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,008] INFO Client environment:java.version=1.8.0_362 (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,008] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,008] INFO Client environment:java.home=/usr/lib/jvm/zulu8-ca-amd64/jre (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,009] INFO Client environment:java.class.path=/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/log4j/driver:/databricks/hive/conf:/databricks/spark/dbconf/hadoop:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.3_2.12---216670720--com.microsoft.azure__azure-annotations__1.2.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.javassist--javassist--org.javassist__javassist__3.25.0-GA.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.scalatest--scalatest_2.12--org.scalatest__scalatest_2.12__3.0.8.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common---317441679--com.google.apis__google-api-services-storage__v1-rev20201112-1.31.0.jar:/databricks/jars/third_party--azure--azure-client-runtime_container_shaded---357863672--com.microsoft.rest__client-runtime__1.7.12.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.3_2.12--1551017351--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2.jar:/databricks/jars/common--spark--version--version-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---1732777613--io.netty__netty-tcnative-boringssl-static__2.0.52.Final.jar:/databricks/jars/common--rcp--rcp-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--jsonwebtoken--jsonwebtoken_shaded---1593727658--com.fasterxml.jackson.core__jackson-annotations__2.9.10.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12--1335359081--org.bouncycastle__bcprov-jdk15on__1.70.jar:/databricks/jars/----ws_3_3--vendor--redshift--libredshift_resources.jar:/databricks/jars/third_party--armeria--armeria_shaded---1544760773--io.micrometer__micrometer-core__1.8.5.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.1.0.jar:/databricks/jars/----ws_3_3--vendor--protobuf--wire-schema-jvm_only_shaded-for-protobuf-hive-2.3__hadoop-3.2--464254218--com.squareup.wire__wire-schema-jvm__4.3.0.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded--985403266--com.google.auth__google-auth-library-credentials__1.15.0.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common--1456152860--io.grpc__grpc-alts__1.37.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.arrow--arrow-memory-netty--org.apache.arrow__arrow-memory-netty__7.0.0.jar:/databricks/jars/third_party--opencensus-shaded--opencensus_shaded---757783023--com.google.errorprone__error_prone_annotations__2.3.4.jar:/databricks/jars/----com_google_protobuf--descriptor_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/common--instrumentation--error-state-recorder--error-state-recorder-spark_3.3_2.12_deploy.jar:/databricks/jars/common--error-framework--libcommon_resources.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.12.189.jar:/databricks/jars/common--encryption--cpk-deps-shaded--2094972470--com.fasterxml.jackson.core__jackson-databind__2.12.7.1.jar:/databricks/jars/proto--logs--service-request--joined_service_request_log_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/common--jobs--job_cluster_key-spark_3.3_2.12_deploy.jar:/databricks/jars/common--jupyter-utils--jupyter_utils-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common--1307550018--org.apache.httpcomponents__httpclient__4.5.13.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---282234195--com.google.guava__failureaccess__1.0.1.jar:/databricks/jars/third_party--scalapb-090--grpc_shaded_scala_2.12--1989328429--com.lihaoyi__fastparse_2.12__2.1.3.jar:/databricks/jars/proto--logs--activity--request_activity_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.glassfish.hk2.external--jakarta.inject--org.glassfish.hk2.external__jakarta.inject__2.6.1.jar:/databricks/jars/common--database--utils--migration-utils-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.hive--hive-shims--org.apache.hive__hive-shims__2.3.9.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.3_2.12---574220505--io.reactivex__rxjava__1.2.4.jar:/databricks/jars/common--encryption--cpk-deps-shaded--1559370686--io.netty__netty-transport-classes-kqueue__4.1.87.Final.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---1312913098--com.google.j2objc__j2objc-annotations__1.3.jar:/databricks/jars/----ws_3_3--vendor--connect--server--spark_connect_proto_lib_shaded---1367335563--com.google.guava__failureaccess__1.0.1.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.13.29.jar:/databricks/jars/----ws_3_3--vendor--protobuf--kafka-schema-registry-client_only_shaded-for-protobuf-hive-2.3__hadoop-3.2--1828902301--io.confluent__kafka-schema-registry-client__7.3.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.codehaus.janino--janino--org.codehaus.janino__janino__3.0.16.jar:/databricks/jars/common--conf--blacklist--blacklist-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.hive--hive-serde--org.apache.hive__hive-serde__2.3.9.jar:/databricks/jars/compliance--taxonomy--taxonomy_cross_scala_lib-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--opencensus-shaded--opencensus_shaded--1770584804--com.lmax__disruptor__3.4.2.jar:/databricks/jars/common--database--datasource--nestedconnection--nestedconnection-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.3_2.12--413473183--com.squareup.okio__okio__1.8.0.jar:/databricks/jars/third_party--jsonwebtoken--jsonwebtoken_shaded--357284269--com.fasterxml.jackson.core__jackson-core__2.9.10.jar:/databricks/jars/third_party--armeria--armeria_shaded---448024057--io.grpc__grpc-protobuf__1.45.1.jar:/databricks/jars/common--rpc--error-details--error-details-holder-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/common--util--with-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.logging.log4j--log4j-slf4j-impl--org.apache.logging.log4j__log4j-slf4j-impl__2.18.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.hive--hive-llap-client--org.apache.hive__hive-llap-client__2.3.9.jar:/databricks/jars/common--common-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--io.netty--netty-transport-native-epoll-linux-x86_64--io.netty__netty-transport-native-epoll-linux-x86_64__4.1.74.Final.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--2124613692--software.amazon.awssdk__regions__2.17.190.jar:/databricks/jars/common--logging--structured--proto-logger-configuration-spark_3.3_2.12_deploy.jar:/databricks/jars/extern--libaws-regions.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--stax--stax-api--stax__stax-api__1.0.1.jar:/databricks/jars/common--util--remote-command-helper-spark_3.3_2.12_deploy.jar:/databricks/jars/common--rpc--metrics--client-metrics-recorder-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__4.1.17.jar:/databricks/jars/proto--logs--activity--dummyservice_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.yetus--audience-annotations--org.apache.yetus__audience-annotations__0.13.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.3_2.12--1340455310--com.google.inject__guice__3.0.jar:/databricks/jars/common--annotations--command-context-py4jwhitelist--command-context-py4jwhitelist-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--armeria--armeria_shaded--951255279--com.google.guava__guava__31.0.1-android.jar:/databricks/jars/common--logging--tags--hadoop--hadoop-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--1126589261--software.amazon.awssdk__profiles__2.17.190.jar:/databricks/jars/common--rate-limiter--rate-limiter-spark_3.3_2.12_deploy.jar:/databricks/jars/common--logging--tags--query-insights--query-insights-spark_3.3_2.12_deploy.jar:/databricks/jars/compliance--redaction--proto-redactor--proto-redactor-spark_3.3_2.12_deploy.jar:/databricks/jars/common--database--circuit-breaker--circuit-breaker-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--opencensus-shaded--opencensus_shaded--111904520--org.apache.thrift__libthrift__0.11.0.jar:/databricks/jars/third_party--azure--azure-storage_shaded--14444357--com.fasterxml.jackson.core__jackson-core__2.14.1.jar:/databricks/jars/----ws_3_3--vendor--connect--server--spark_connect_proto_lib_shaded---346995935--io.netty__netty-common__4.1.77.Final.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk---1091084412--org.apache.kafka__connect-api__2.5.0.jar:/databricks/jars/feature-flag--client--client-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12---407334024--io.projectreactor.netty__reactor-netty-core__1.0.20.jar:/databricks/jars/----ws_3_3--vendor--protobuf--protobuf-java_shaded-for-protobuf-hive-2.3__hadoop-3.2--783906070--com.google.protobuf__protobuf-java__3.21.9.jar:/databricks/jars/proto--logs-catalog--internal_log_metadata_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---1824216449--com.google.protobuf__protobuf-java-util__3.15.5.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.36.jar:/databricks/jars/common--principal-context--api--principal_context_scala_proto-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---235794917--io.grpc__grpc-protobuf__1.37.0.jar:/databricks/jars/common--rpc--validator--validator-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--vendor--connect--server--spark_connect_proto_lib_shaded--1596312874--io.grpc__grpc-stub__1.45.1.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---330209066--io.netty__netty-codec__4.1.77.Final.jar:/databricks/jars/daemon--data--client--utils-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.orc--orc-mapreduce--org.apache.orc__orc-mapreduce__1.7.8.jar:/databricks/jars/common--hadoop--hadoop-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--sql--core--proto_2.12_deploy.jar:/databricks/jars/----ws_3_3--patched-hive-with-glue--hive-common_filtered--1003103642--org.apache.hive__hive-common__2.3.9.jar:/databricks/jars/spark--conf-reader--conf-reader_lib-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--vendor--connect--server--spark_connect_proto_lib_shaded---1254772955--io.grpc__grpc-core__1.45.1.jar:/databricks/jars/third_party--azure--azure-storage_shaded--595225951--org.apache.commons__commons-lang3__3.4.jar:/databricks/jars/spark--driver--driver-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--vendor--protobuf--protobuf-java-util_shaded-for-protobuf-hive-2.3__hadoop-3.2---333227431--com.google.protobuf__protobuf-java-util__3.19.4.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.13.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.12.189.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.3_2.12---1345432102--commons-codec__commons-codec__1.11.jar:/databricks/jars/----ws_3_3--mllib-local--mllib-local-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded--19273035--com.google.flogger__flogger__0.5.1.jar:/databricks/jars/compliance--taxonomy--taxonomy_proto_library-spark_3.3_2.12-scalabp.jar:/databricks/jars/common--conf--dbconf--dbconf-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.12.189.jar:/databricks/jars/common--http--tag-definition-parser--tag-definition-parser-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/common--advanced-feature--advanced-feature-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---13112641--org.codehaus.mojo__animal-sniffer-annotations__1.19.jar:/databricks/jars/third_party--opencensus-shaded--opencensus_shaded---439494304--io.opentracing__opentracing-noop__0.31.0.jar:/databricks/jars/----glue-catalog-spark3.3-client--glue-catalog-client-common_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--redshift-driver--redshift-driver-v2_shaded---983116245--com.amazon.redshift__redshift-jdbc42__2.1.0.4.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.glassfish.hk2--hk2-api--org.glassfish.hk2__hk2-api__2.6.1.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.12.189.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.12.189.jar:/databricks/jars/common--encryption--cpk-deps-shaded---1862756090--com.nimbusds__oauth2-oidc-sdk__9.7.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.13.4.jar:/databricks/jars/common--logging--structured--catalog-spark_3.3_2.12_deploy.jar:/databricks/jars/common--util--string-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common---1524799217--org.checkerframework__checker-qual__3.5.0.jar:/databricks/jars/common--instrumentation--info-metrics--metrics-spark_3.3_2.12_deploy.jar:/databricks/jars/common--conf--global--global-spark_3.3_2.12_deploy.jar:/databricks/jars/common--logging--filters--filters-spark_3.3_2.12_deploy.jar:/databricks/jars/spark--driver--antlr--parser-spark_3.3_2.12_deploy.jar:/databricks/jars/common--conf--cluster-spark--cluster-spark-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12---595340659--net.minidev__accessors-smart__2.4.8.jar:/databricks/jars/common--driver-nfs--driver-nfs-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--armeria--armeria_shaded---1125725645--io.netty__netty-transport__4.1.77.Final.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---1059575973--com.google.guava__guava__30.1-jre.jar:/databricks/jars/proto--logs--service-request--service_request_log_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded--1794663302--com.google.flogger__flogger-system-backend__0.5.1.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--19901040--software.amazon.awssdk__aws-query-protocol__2.17.190.jar:/databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--jsonwebtoken--jsonwebtoken_shaded---639136574--io.jsonwebtoken__jjwt-impl__0.10.5.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--io.delta--delta-sharing-spark_2.12--io.delta__delta-sharing-spark_2.12__0.6.9.jar:/databricks/jars/----ws_3_3--vendor--connect--server--spark_connect_proto_lib_shaded---1275080308--com.google.code.gson__gson__2.8.9.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/common--encryption--cpk-deps-shaded--111294749--com.azure__azure-core__1.19.0.jar:/databricks/jars/common--logging--service-request--builder-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--vendor--connect--server--spark_connect_proto_lib_shaded--619126430--io.perfmark__perfmark-api__0.23.0.jar:/databricks/jars/common--database--migration--flag-reader--flag-reader-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--206005473--software.amazon.awssdk__annotations__2.17.190.jar:/databricks/jars/common--instrumentation--info-service--info-service-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.12.189.jar:/databricks/jars/common--tracing--util--span_util-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--1674597699--software.amazon.kinesis__amazon-kinesis-client__2.3.4.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded--185812052--io.perfmark__perfmark-api__0.23.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.12.189.jar:/databricks/jars/common--conf--spark-configs-validator--spark-configs-validator-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.spark-project.spark--unused--org.spark-project.spark__unused__1.0.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.12.0.jar:/databricks/jars/spark--hadoop-safety--filesystem_list_2.12_deploy.jar:/databricks/jars/common--conf--base-spark_3.3_2.12_deploy.jar:/databricks/jars/common--storage--storage-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.hive--hive-llap-common--org.apache.hive__hive-llap-common__2.3.9.jar:/databricks/jars/common--logging--policies--policies-log4j2-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--io.netty--netty-transport-native-epoll-linux-aarch_64--io.netty__netty-transport-native-epoll-linux-aarch_64__4.1.74.Final.jar:/databricks/jars/----ws_3_3--vendor--connect--server--spark_connect_proto_lib_shaded--1805148092--io.netty__netty-resolver__4.1.77.Final.jar:/databricks/jars/common--encryption--cpk-deps-shaded---577814894--io.projectreactor.netty__reactor-netty-core__1.0.9.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded--1669633003--com.google.http-client__google-http-client-gson__1.39.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.3_2.12---364933132--com.squareup.okhttp3__okhttp-urlconnection__3.3.1.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--1365248287--io.netty__netty-codec-http2__4.1.77.Final.jar:/databricks/jars/common--logging--log4j--log4j-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common--1121278521--com.google.api.grpc__proto-google-iam-v1__1.0.5.jar:/databricks/jars/api--rpc--rpc_parser-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--armeria--armeria_shaded---346995935--io.netty__netty-common__4.1.77.Final.jar:/databricks/jars/common--encryption--cpk-deps-shaded--1200635519--com.azure__azure-security-keyvault-keys__4.3.2.jar:/databricks/jars/common--conf--parser--parser-spark_3.3_2.12_deploy.jar:/databricks/jars/common--hadoop--debugtools--debugtools-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:/databricks/jars/api-base--proto--error_code_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12--252001068--io.netty__netty-tcnative-boringssl-static__2.0.53.Final.jar:/databricks/jars/common--instrumentation--cache-exporter--cache-exporter-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded--705909857--com.google.http-client__google-http-client-jackson2__1.39.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.12.189.jar:/databricks/jars/----ws_3_3--safespark--udf--common--udf_grpc-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/common--util--service-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--armeria--armeria_shaded---210480075--io.netty__netty-handler-proxy__4.1.77.Final.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---1843136381--com.google.apis__google-api-services-networkmanagement__v1-rev20200505-1.30.10.jar:/databricks/jars/third_party--jackson--shaded_dependencies---2005166618--com.google.guava__guava__15.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.roaringbitmap--RoaringBitmap--org.roaringbitmap__RoaringBitmap__0.9.25.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.slf4j--jcl-over-slf4j--org.slf4j__jcl-over-slf4j__1.7.36.jar:/databricks/jars/common--conf--spark-version--spark-version-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---931817476--io.grpc__grpc-stub__1.34.1.jar:/databricks/jars/----ws_3_3--vendor--avro--avro-hive-2.3__hadoop-3.2_2.12_shaded---1954496799--avro-unshaded-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/common--encryption--cpk-deps-shaded---1955934530--io.netty__netty-handler-proxy__4.1.87.Final.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-connector-all-shaded-spark_3.3_2.12--1057051737--gcsio_proto_library-speed-src.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.wildfly.openssl--wildfly-openssl--org.wildfly.openssl__wildfly-openssl__1.0.7.Final.jar:/databricks/jars/----ws_3_3--sql--hive-thriftserver--hive-thriftserver-protocol-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.json4s--json4s-core_2.12--org.json4s__json4s-core_2.12__3.7.0-M11.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.12.0.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded--533334041--io.netty__netty-tcnative-boringssl-static-linux-aarch_64__2.0.52.Final.jar:/databricks/jars/\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n/bin/../libs/lz4-java-1.8.0.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/maven-artifact-3.8.8.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/metrics-core-2.2.0.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/metrics-core-4.1.12.1.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-buffer-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-codec-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-common-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-handler-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-resolver-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-transport-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-transport-classes-epoll-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-transport-native-epoll-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-transport-native-unix-common-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/osgi-resource-locator-1.0.3.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/paranamer-2.8.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/plexus-utils-3.3.1.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/reflections-0.9.12.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/reload4j-1.2.25.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/rocksdbjni-7.1.2.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/scala-collection-compat_2.12-2.10.0.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/scala-java8-compat_2.12-1.0.2.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/scala-library-2.12.15.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/scala-logging_2.12-3.9.4.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/scala-reflect-2.12.15.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/slf4j-api-1.7.36.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/slf4j-reload4j-1.7.36.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/snappy-java-1.1.10.1.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/swagger-annotations-2.2.8.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/trogdor-3.5.1.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/zookeeper-3.6.4.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/zookeeper-jute-3.6.4.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/zstd-jni-1.5.5-1.jar (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,029] INFO Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,029] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,030] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,030] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,030] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,030] INFO Client environment:os.version=5.15.0-1043-aws (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,030] INFO Client environment:user.name=root (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,030] INFO Client environment:user.home=/root (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,031] INFO Client environment:user.dir=/databricks/driver (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,031] INFO Client environment:os.memory.free=1004MB (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,031] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,031] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,066] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@1edb61b1 (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 23:14:53,108] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)\n[2023-10-04 23:14:53,132] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)\n[2023-10-04 23:14:53,137] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)\n[2023-10-04 23:14:53,145] INFO Opening socket connection to server localhost/127.0.0.1:2181. (org.apache.zookeeper.ClientCnxn)\n[2023-10-04 23:14:53,162] INFO Socket connection established, initiating session, client: /127.0.0.1:39634, server: localhost/127.0.0.1:2181 (org.apache.zookeeper.ClientCnxn)\n[2023-10-04 23:14:53,316] INFO Session establishment complete on server localhost/127.0.0.1:2181, session id = 0x1000225e1260000, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)\n[2023-10-04 23:14:53,345] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)\n[2023-10-04 23:14:55,031] INFO Cluster ID = E2P6_jhcS-O2SFMc31d4EQ (kafka.server.KafkaServer)\n[2023-10-04 23:14:55,045] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)\n[2023-10-04 23:14:55,293] INFO KafkaConfig values: \n\tadvertised.listeners = null\n\talter.config.policy.class.name = null\n\talter.log.dirs.replication.quota.window.num = 11\n\talter.log.dirs.replication.quota.window.size.seconds = 1\n\tauthorizer.class.name = \n\tauto.create.topics.enable = true\n\tauto.include.jmx.reporter = true\n\tauto.leader.rebalance.enable = true\n\tbackground.threads = 10\n\tbroker.heartbeat.interval.ms = 2000\n\tbroker.id = 0\n\tbroker.id.generation.enable = true\n\tbroker.rack = null\n\tbroker.session.timeout.ms = 9000\n\tclient.quota.callback.class = null\n\tcompression.type = producer\n\tconnection.failed.authentication.delay.ms = 100\n\tconnections.max.idle.ms = 600000\n\tconnections.max.reauth.ms = 0\n\tcontrol.plane.listener.name = null\n\tcontrolled.shutdown.enable = true\n\tcontrolled.shutdown.max.retries = 3\n\tcontrolled.shutdown.retry.backoff.ms = 5000\n\tcontroller.listener.names = null\n\tcontroller.quorum.append.linger.ms = 25\n\tcontroller.quorum.election.backoff.max.ms = 1000\n\tcontroller.quorum.election.timeout.ms = 1000\n\tcontroller.quorum.fetch.timeout.ms = 2000\n\tcontroller.quorum.request.timeout.ms = 2000\n\tcontroller.quorum.retry.backoff.ms = 20\n\tcontroller.quorum.voters = []\n\tcontroller.quota.window.num = 11\n\tcontroller.quota.window.size.seconds = 1\n\tcontroller.socket.timeout.ms = 30000\n\tcreate.topic.policy.class.name = null\n\tdefault.replication.factor = 1\n\tdelegation.token.expiry.check.interval.ms = 3600000\n\tdelegation.token.expiry.time.ms = 86400000\n\tdelegation.token.master.key = null\n\tdelegation.token.max.lifetime.ms = 604800000\n\tdelegation.token.secret.key = null\n\tdelete.records.purgatory.purge.interval.requests = 1\n\tdelete.topic.enable = true\n\tearly.start.listeners = null\n\tfetch.max.bytes = 57671680\n\tfetch.purgatory.purge.interval.requests = 1000\n\tgroup.consumer.assignors = []\n\tgroup.consumer.heartbeat.interval.ms = 5000\n\tgroup.consumer.max.heartbeat.interval.ms = 15000\n\tgroup.consumer.max.session.timeout.ms = 60000\n\tgroup.consumer.max.size = 2147483647\n\tgroup.consumer.min.heartbeat.interval.ms = 5000\n\tgroup.consumer.min.session.timeout.ms = 45000\n\tgroup.consumer.session.timeout.ms = 45000\n\tgroup.coordinator.new.enable = false\n\tgroup.coordinator.threads = 1\n\tgroup.initial.rebalance.delay.ms = 0\n\tgroup.max.session.timeout.ms = 1800000\n\tgroup.max.size = 2147483647\n\tgroup.min.session.timeout.ms = 6000\n\tinitial.broker.registration.timeout.ms = 60000\n\tinter.broker.listener.name = null\n\tinter.broker.protocol.version = 3.5-IV2\n\tkafka.metrics.polling.interval.secs = 10\n\tkafka.metrics.reporters = []\n\tleader.imbalance.check.interval.seconds = 300\n\tleader.imbalance.per.broker.percentage = 10\n\tlistener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n\tlisteners = PLAINTEXT://:9092\n\tlog.cleaner.backoff.ms = 15000\n\tlog.cleaner.dedupe.buffer.size = 134217728\n\tlog.cleaner.delete.retention.ms = 86400000\n\tlog.cleaner.enable = true\n\tlog.cleaner.io.buffer.load.factor = 0.9\n\tlog.cleaner.io.buffer.size = 524288\n\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n\tlog.cleaner.min.cleanable.ratio = 0.5\n\tlog.cleaner.min.compaction.lag.ms = 0\n\tlog.cleaner.threads = 1\n\tlog.cleanup.policy = [delete]\n\tlog.dir = /tmp/kafka-logs\n\tlog.dirs = /tmp/kafka-logs\n\tlog.flush.interval.messages = 9223372036854775807\n\tlog.flush.interval.ms = null\n\tlog.flush.offset.checkpoint.interval.ms = 60000\n\tlog.flush.scheduler.interval.ms = 9223372036854775807\n\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n\tlog.index.interval.bytes = 4096\n\tlog.index.size.max.bytes = 10485760\n\tlog.message.downconversion.enable = true\n\tlog.message.format.version = 3.0-IV1\n\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n\tlog.message.timestamp.type = CreateTime\n\tlog.preallocate = false\n\tlog.retention.bytes = -1\n\tlog.retention.check.interval.ms = 300000\n\tlog.retention.hours = 168\n\tlog.retention.minutes = null\n\tlog.retention.ms = null\n\tlog.roll.hours = 168\n\tlog.roll.jitter.hours = 0\n\tlog.roll.jitter.ms = null\n\tlog.roll.ms = null\n\tlog.segment.bytes = 1073741824\n\tlog.segment.delete.delay.ms = 60000\n\tmax.connection.creation.rate = 2147483647\n\tmax.connections = 2147483647\n\tmax.connections.per.ip = 2147483647\n\tmax.connections.per.ip.overrides = \n\tmax.incremental.fetch.session.cache.slots = 1000\n\tmessage.max.bytes = 1048588\n\tmetadata.log.dir = null\n\tmetadata.log.max.record.bytes.between.snapshots = 20971520\n\tmetadata.log.max.snapshot.interval.ms = 3600000\n\tmetadata.log.segment.bytes = 1073741824\n\tmetadata.log.segment.min.bytes = 8388608\n\tmetadata.log.segment.ms = 604800000\n\tmetadata.max.idle.interval.ms = 500\n\tmetadata.max.retention.bytes = 104857600\n\tmetadata.max.retention.ms = 604800000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\tmin.insync.replicas = 1\n\tnode.id = 0\n\tnum.io.threads = 8\n\tnum.network.threads = 3\n\tnum.partitions = 1\n\tnum.recovery.threads.per.data.dir = 1\n\tnum.replica.alter.log.dirs.threads = null\n\tnum.replica.fetchers = 1\n\toffset.metadata.max.bytes = 4096\n\toffsets.commit.required.acks = -1\n\toffsets.commit.timeout.ms = 5000\n\toffsets.load.buffer.size = 5242880\n\toffsets.retention.check.interval.ms = 600000\n\toffsets.retention.minutes = 10080\n\toffsets.topic.compression.codec = 0\n\toffsets.topic.num.partitions = 50\n\toffsets.topic.replication.factor = 1\n\toffsets.topic.segment.bytes = 104857600\n\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n\tpassword.encoder.iterations = 4096\n\tpassword.encoder.key.length = 128\n\tpassword.encoder.keyfactory.algorithm = null\n\tpassword.encoder.old.secret = null\n\tpassword.encoder.secret = null\n\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\n\tprocess.roles = []\n\tproducer.id.expiration.check.interval.ms = 600000\n\tproducer.id.expiration.ms = 86400000\n\tproducer.purgatory.purge.interval.requests = 1000\n\tqueued.max.request.bytes = -1\n\tqueued.max.requests = 500\n\tquota.window.num = 11\n\tquota.window.size.seconds = 1\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.manager.class.name = null\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = null\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = null\n\tremote.log.storage.system.enable = false\n\treplica.fetch.backoff.ms = 1000\n\treplica.fetch.max.bytes = 1048576\n\treplica.fetch.min.bytes = 1\n\treplica.fetch.response.max.bytes = 10485760\n\treplica.fetch.wait.max.ms = 500\n\treplica.high.watermark.checkpoint.interval.ms = 5000\n\treplica.lag.time.max.ms = 30000\n\treplica.selector.class = null\n\treplica.socket.receive.buffer.bytes = 65536\n\treplica.socket.timeout.ms = 30000\n\treplication.quota.window.num = 11\n\treplication.quota.window.size.seconds = 1\n\trequest.timeout.ms = 30000\n\treserved.broker.max.id = 1000\n\tsasl.client.callback.handler.class = null\n\tsasl.enabled.mechanisms = [GSSAPI]\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism.controller.protocol = GSSAPI\n\tsasl.mechanism.inter.broker.protocol = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsasl.server.callback.handler.class = null\n\tsasl.server.max.receive.size = 524288\n\tsecurity.inter.broker.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tserver.max.startup.time.ms = 9223372036854775807\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tsocket.listen.backlog.size = 50\n\tsocket.receive.buffer.bytes = 102400\n\tsocket.request.max.bytes = 104857600\n\tsocket.send.buffer.bytes = 102400\n\tssl.cipher.suites = []\n\tssl.client.auth = none\n\tssl.enabled.protocols = [TLSv1.2]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.principal.mapping.rules = DEFAULT\n\tssl.protocol = TLSv1.2\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n\ttransaction.max.timeout.ms = 900000\n\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n\ttransaction.state.log.load.buffer.size = 5242880\n\ttransaction.state.log.min.isr = 1\n\ttransaction.state.log.num.partitions = 50\n\ttransaction.state.log.replication.factor = 1\n\ttransaction.state.log.segment.bytes = 104857600\n\ttransactional.id.expiration.ms = 604800000\n\tunclean.leader.election.enable = false\n\tunstable.api.versions.enable = false\n\tzookeeper.clientCnxnSocket = null\n\tzookeeper.connect = localhost:2181\n\tzookeeper.connection.timeout.ms = 18000\n\tzookeeper.max.in.flight.requests = 10\n\tzookeeper.metadata.migration.enable = false\n\tzookeeper.session.timeout.ms = 18000\n\tzookeeper.set.acl = false\n\tzookeeper.ssl.cipher.suites = null\n\tzookeeper.ssl.client.enable = false\n\tzookeeper.ssl.crl.enable = false\n\tzookeeper.ssl.enabled.protocols = null\n\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n\tzookeeper.ssl.keystore.location = null\n\tzookeeper.ssl.keystore.password = null\n\tzookeeper.ssl.keystore.type = null\n\tzookeeper.ssl.ocsp.enable = false\n\tzookeeper.ssl.protocol = TLSv1.2\n\tzookeeper.ssl.truststore.location = null\n\tzookeeper.ssl.truststore.password = null\n\tzookeeper.ssl.truststore.type = null\n (kafka.server.KafkaConfig)\n[2023-10-04 23:14:55,728] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2023-10-04 23:14:55,739] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2023-10-04 23:14:55,758] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2023-10-04 23:14:55,796] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2023-10-04 23:14:55,964] INFO Log directory /tmp/kafka-logs not found, creating it. (kafka.log.LogManager)\n[2023-10-04 23:14:56,157] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)\n[2023-10-04 23:14:56,205] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\n[2023-10-04 23:14:56,308] INFO Loaded 0 logs in 150ms (kafka.log.LogManager)\n[2023-10-04 23:14:56,359] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\n[2023-10-04 23:14:56,363] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\n[2023-10-04 23:14:56,600] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\n[2023-10-04 23:14:56,697] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n[2023-10-04 23:14:56,753] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)\n[2023-10-04 23:14:56,905] INFO [zk-broker-0-to-controller-forwarding-channel-manager]: Starting (kafka.server.BrokerToControllerRequestThread)\n[2023-10-04 23:14:59,403] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\n[2023-10-04 23:14:59,510] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\n[2023-10-04 23:14:59,558] INFO [zk-broker-0-to-controller-alter-partition-channel-manager]: Starting (kafka.server.BrokerToControllerRequestThread)\n[2023-10-04 23:14:59,700] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 23:14:59,706] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 23:14:59,710] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 23:14:59,721] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 23:14:59,816] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\n[2023-10-04 23:14:59,906] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)\n[2023-10-04 23:15:00,074] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1696461300029,1696461300029,1,0,0,72059955752337408,254,0,25\n (kafka.zk.KafkaZkClient)\n[2023-10-04 23:15:00,076] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://1004-210429-gf1iljoc-10-172-194-117:9092, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient)\n[2023-10-04 23:15:00,413] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 23:15:00,488] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 23:15:00,501] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)\n[2023-10-04 23:15:00,499] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 23:15:00,608] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)\n[2023-10-04 23:15:00,640] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)\n[2023-10-04 23:15:00,683] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\n[2023-10-04 23:15:00,860] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\n[2023-10-04 23:15:00,881] INFO [TxnMarkerSenderThread-0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n[2023-10-04 23:15:00,882] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\n[2023-10-04 23:15:00,924] INFO [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0). (kafka.server.metadata.ZkMetadataCache)\n[2023-10-04 23:15:01,485] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 23:15:01,625] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 23:15:01,664] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (1004-210429-gf1iljoc-10-172-194-117/127.0.1.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 23:15:01,709] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 23:15:01,812] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 23:15:01,815] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (1004-210429-gf1iljoc-10-172-194-117/127.0.1.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 23:15:01,819] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 23:15:01,919] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n[2023-10-04 23:15:01,927] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 23:15:01,927] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (1004-210429-gf1iljoc-10-172-194-117/127.0.1.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 23:15:01,932] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 23:15:01,966] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing. (kafka.network.SocketServer)\n[2023-10-04 23:15:01,976] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\n[2023-10-04 23:15:02,237] INFO Kafka version: 7.4.0-ccs (org.apache.kafka.common.utils.AppInfoParser)\n[2023-10-04 23:15:02,238] INFO Kafka commitId: 30969fa33c185e88 (org.apache.kafka.common.utils.AppInfoParser)\n[2023-10-04 23:15:02,238] INFO Kafka startTimeMs: 1696461302128 (org.apache.kafka.common.utils.AppInfoParser)\n[2023-10-04 23:15:02,270] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)\n[2023-10-04 23:15:02,803] INFO [zk-broker-0-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node 1004-210429-gf1iljoc-10-172-194-117:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n[2023-10-04 23:15:02,804] INFO [zk-broker-0-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node 1004-210429-gf1iljoc-10-172-194-117:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n[2023-10-04 23:15:20,409] INFO Creating topic responseManualRequest with configuration {retention.ms=3600000} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)\n[2023-10-04 23:15:20,734] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(responseManualRequest-0) (kafka.server.ReplicaFetcherManager)\n[2023-10-04 23:15:20,956] INFO [LogLoader partition=responseManualRequest-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2023-10-04 23:15:21,014] INFO Created log for partition responseManualRequest-0 in /tmp/kafka-logs/responseManualRequest-0 with properties {retention.ms=3600000} (kafka.log.LogManager)\n[2023-10-04 23:15:21,019] INFO [Partition responseManualRequest-0 broker=0] No checkpointed highwatermark is found for partition responseManualRequest-0 (kafka.cluster.Partition)\n[2023-10-04 23:15:21,026] INFO [Partition responseManualRequest-0 broker=0] Log loaded for partition responseManualRequest-0 with initial high watermark 0 (kafka.cluster.Partition)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh \n",
    "./kafka_2.12-3.5.1/bin/kafka-server-start.sh ./kafka_2.12-3.5.1/config/server.properties"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1014980191216800,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2.SERVIDOR_KAFKA",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
