{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a9dcf4-32b4-4f89-85d9-c4330ba71076",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Neste notebook é feita a configuração do KafkaConsumer, que receberá a informação enviada pelo Webhook e salvará a informação na mesma tabela que é feito o salvamento das requisições automáticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe098dc-ab98-4b03-a7b5-1c1072f6f198",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting kafka-python\n  Using cached kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\nInstalling collected packages: kafka-python\nSuccessfully installed kafka-python-2.0.2\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "781acc41-d233-478b-bfae-ebf486ea0fa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import kafka\n",
    "from kafka import KafkaConsumer\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "from pyspark.sql.functions import col \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faa58fc9-fa7c-4fb4-a88a-41670f0b9ce9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data consumed! Total NEW articles found: 1\nNew articles loaded in [/FileStore/tables/table_articles]\n---------------------------------------------------------\nData consumed! Total NEW articles found: 0\nNew articles loaded in [/FileStore/tables/table_articles]\n---------------------------------------------------------\nData consumed! Total NEW articles found: 0\nNew articles loaded in [/FileStore/tables/table_articles]\n---------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MINUTES_TO_RUN_KAFKA_REPORT = ['5', '10', '15', '20', '25', '35', '40', '45', '50', '55']\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('author', StringType(), True),\n",
    "    StructField('content', StringType(), True),\n",
    "    StructField('description', StringType(), True),\n",
    "    StructField('publishedAt', StringType(), True),\n",
    "    StructField('source', MapType(StringType(), StringType(), True), True),\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('url', StringType(), True),\n",
    "    StructField('urlToImage', StringType(), True)\n",
    "    ])\n",
    "\n",
    "def data_save():\n",
    "    consumer = KafkaConsumer('responseManualRequest', bootstrap_servers=['localhost:9092'],auto_offset_reset='earliest', consumer_timeout_ms=10000)\n",
    "    kafka_consumer_report = []\n",
    "    for message in consumer:\n",
    "        report_dict = json.loads(json.loads(message.value.decode()))\n",
    "        kafka_consumer_report.extend(report_dict['articles'])\n",
    "\n",
    "    return kafka_consumer_report\n",
    "\n",
    "while True:\n",
    "    if str(datetime.now().minute) in MINUTES_TO_RUN_KAFKA_REPORT:\n",
    "        kafka_consumer_report = data_save()\n",
    "        df_report_kafka = spark.createDataFrame(kafka_consumer_report, schema=schema)\n",
    "        df_report_kafka = df_report_kafka.withColumn('source', col('source.name')) # <- this overwrites the colum \"source\" (type:map) to make a colum\n",
    "                                                                                # (type:str) with only the name of the source (\"source id\" deleted)\n",
    "        if spark.catalog.tableExists('table_articles'):\n",
    "            df_temp = spark.read.parquet('/FileStore/tables/table_articles')\n",
    "            df_report_kafka = df_report_kafka.subtract(df_temp)\n",
    "\n",
    "        df_report_kafka = df_report_kafka.filter(~col('source').contains('[Removed]'))\n",
    "        df_report_kafka = df_report_kafka.dropDuplicates()\n",
    "        df_report_kafka.write.mode('append').format('parquet').option(\n",
    "        'path', '/FileStore/tables/table_articles').saveAsTable('table_articles')\n",
    "        \n",
    "        print(f'Data consumed! Total NEW articles found: {df_report_kafka.count()}')\n",
    "        print(f'New articles loaded in [/FileStore/tables/table_articles]')\n",
    "        print('---------------------------------------------------------')\n",
    "        \n",
    "        time.sleep(100) # 100 seconds\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3361616726341782,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "6.KAFKA_CONSUMER",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
